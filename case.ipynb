{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nubank case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In email backlogs, the arrival date refers to the begining of the week (since every date there refers to a Monday)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:30:12.835920Z",
     "start_time": "2019-06-19T20:30:02.485056Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_datetime(df, dt_col_name):\n",
    "    '''Given a dataframe and a date/datetime column, decompose the column in its components'''\n",
    "    df[dt_col_name+'__YEAR'] = df[dt_col_name].dt.year\n",
    "    df[dt_col_name+'__MONTH'] = df[dt_col_name].dt.month\n",
    "    df[dt_col_name+'__DAY'] = df[dt_col_name].dt.day\n",
    "    df[dt_col_name+'__WDAY'] = df[dt_col_name].dt.weekday_name.apply(lambda x: x[:3])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:30:40.604397Z",
     "start_time": "2019-06-19T20:30:40.523401Z"
    }
   },
   "outputs": [],
   "source": [
    "efile = pd.read_excel('./dataset.xlsx', sheet_name=None)\n",
    "efile.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:40:51.674069Z",
     "start_time": "2019-06-19T20:40:51.558069Z"
    }
   },
   "outputs": [],
   "source": [
    "hourly = pd.read_excel('./dataset.xlsx', sheet_name='hourly', header=2)\n",
    "hourly\n",
    "h = pd.melt(hourly, id_vars='hour/shift')\n",
    "h['channel'] = h['variable'].apply(lambda x: x.split('-')[0])\n",
    "h['day'] = h['variable'].apply(lambda x: x.split('-')[1])\n",
    "#h['time'] = pd.to_datetime(h['hour/shift'], format= '%H:%M:%S').dt.time #Unnecessary, it is already recogizing\n",
    "print(h.dtypes)\n",
    "#h.loc[0,'time']\n",
    "hourly=h\n",
    "hourly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose hourly dataset\n",
    "\n",
    "Our goal here is to relate all relevant information through their temporal dimension.\n",
    "Some data will need to be upsampled, since we have monthly, weekly and hourly data.\n",
    "\n",
    "Let's put everything in a common hourly frequency. Upsampling assumptions must be explicit.\n",
    "\n",
    "At the end of this section, we should have one unique dataframe with hourly frequency containing number of jobs (upsampled considering daily and hourly distribution percentage) and emaill backlogs (upsampled considering daily and hourly distribution percentage).\n",
    "\n",
    "The final frame will still keep data about avg job time and p75 of FRT, but this data does not need resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all dates and times, weekday names, squads and channel names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:32:23.173188Z",
     "start_time": "2019-06-19T20:32:23.168181Z"
    }
   },
   "outputs": [],
   "source": [
    "job_count = efile['Job count']\n",
    "# Month end of date\n",
    "job_count['job_count_date_ME'] = pd.to_datetime(job_count['month'], format='%Y-%m') + pd.offsets.MonthEnd(0)\n",
    "job_count['channel'] = job_count['channel'].replace('inbound_call', 'phone')\n",
    "job_count[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily = efile['daily']\n",
    "daily.day.unique()\n",
    "daily[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "email_blog = efile['Email Backlog']\n",
    "# Since the arrival date is a Monday, this date refers to the begining of the arrival week\n",
    "email_blog['begin_arrived_week_date'] = pd.to_datetime(email_blog['arrived week']).dt.tz_localize(None)\n",
    "email_blog = enhance_datetime(email_blog, 'begin_arrived_week_date')\n",
    "email_blog[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity checks and parameters gathering\n",
    "Test quality and gather important internal parameters (like minimum and maximum dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(hourly['day'])==set(daily['day'])\n",
    "assert set(hourly['channel'])==set(job_count['channel'])==set(daily['channel'])\n",
    "assert set(job_count['squad'])==set(email_blog['squad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(hourly['day']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(hourly['channel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DATE = job_count['job_count_date_ME'].min() + pd.offsets.MonthBegin(-1)\n",
    "MAX_DATE = job_count['job_count_date_ME'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DATE_email = email_blog['begin_arrived_week_date'].min() + pd.offsets.MonthBegin(-1)\n",
    "MAX_DATE_email = email_blog['begin_arrived_week_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert MIN_DATE<=MIN_DATE_email\n",
    "assert MAX_DATE<=MAX_DATE_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_jobs_by_month = job_count[['channel', 'squad', 'job_count_date_ME', 'jobs']].groupby(by=['squad','channel', 'job_count_date_ME'])\n",
    "if total_jobs_by_month.sum().shape[0]==job_count.shape[0]:\n",
    "    print('The job_count totals by squad and channel do correspond to grouping them.')\n",
    "    print('That means, there is no repeated date in the job_count table.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create datetime df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will serve as the base frame to relate every info to a time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate date dataframe\n",
    "dt_df = pd.DataFrame()\n",
    "dt_df['date'] = pd.date_range(MIN_DATE, MAX_DATE)\n",
    "dt_df['date_ME'] = dt_df['date'] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "assert (dt_df['date_ME'].dt.month == dt_df['date'].dt.month).all()\n",
    "\n",
    "dt_df = enhance_datetime(dt_df, 'date')\n",
    "dt_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create month to daily dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For estimating daily jobs (upsample).\n",
    "\n",
    "DETAILS BELOW\n",
    "\n",
    "By upsampling, since not all months have exaclty four weeks, it is necessary to normalize the daily percentages to monthly percentages, which may generate some difference in weekday distribution for weeks that are split in two months. But given the growth rate of jobs, this difference is neglectible and the normalization is a reasonable assumption. If if the number of jobs doubles from one month to another, we would be speaking of this increase diluted in 3 or 4 days in 30.\n",
    "\n",
    "The alternative would be solving a linear system for daily jobs, given monthly totals and in-week percentage distribution as equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with daily\n",
    "df_daily = dt_df.merge(daily, how='left',left_on='date__WDAY', right_on='day')\n",
    "df_daily = df_daily.rename(columns={'percentage':'wday_percentage'})\n",
    "df_daily[:5]\n",
    "\n",
    "# Group by channel, sum percentages for the month and normalize channel percentage by total channel monthly percentage\n",
    "temp = df_daily[['channel','date_ME','wday_percentage']].groupby(by=['date_ME', 'channel']).sum()\n",
    "temp = temp.reset_index().rename(columns={'wday_percentage':'total_month_wday_percentage_by_channel'})\n",
    "temp[:5]\n",
    "\n",
    "# Join totals df_daily and adjust percentage\n",
    "df_daily = df_daily.merge(temp, how='left', left_on=['channel','date_ME'], right_on=['channel','date_ME'])\n",
    "df_daily['month_to_day_percentage_adjusted'] = df_daily['wday_percentage']/df_daily['total_month_wday_percentage_by_channel']\n",
    "df_daily[:5]\n",
    "\n",
    "# Assert total percentages is summing up to 1 for each channel every month\n",
    "res = set((df_daily[['date_ME','channel','month_to_day_percentage_adjusted']]\n",
    " .groupby(by=['date_ME', 'channel']).sum().round()['month_to_day_percentage_adjusted']))\n",
    "assert len(res)==1\n",
    "assert list(res)[0]==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate daily jobs (upsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the job_counts to the date dataframe (df) and resample the monthly total jobs to daily jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_jobs = df_daily.merge(job_count, how='left', left_on=['channel'], right_on=['channel'])\n",
    "df_daily_jobs = df_daily_jobs.rename(columns={'jobs':'jobs_monthly'}) # for the sake of clarity\n",
    "df_daily_jobs['jobs_daily'] = df_daily_jobs['month_to_day_percentage_adjusted']*df_daily_jobs['jobs_monthly']\n",
    "df_daily_jobs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate daily email back logs (upsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the email_blog to the df_daily_jobs and resample the weekly total jobs to daily backlog, by squad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join first with datetime\n",
    "email_blog['channel'] = 'email'\n",
    "templ = []\n",
    "for squad in set(email_blog['squad']):\n",
    "    sli = email_blog[email_blog['squad']==squad].copy()\n",
    "    temp = dt_df.merge(sli, how='left', left_on=['date'], right_on=['begin_arrived_week_date'])\n",
    "    temp = temp.ffill()\n",
    "    temp = temp.rename(columns={'backlog volume':'backlog_volume_of_week'})\n",
    "    templ.append(temp.copy())\n",
    "\n",
    "templ = pd.concat(templ)\n",
    "templ = templ.dropna(subset=['backlog_volume_of_week'])\n",
    "templ = templ[['date', 'arrived week', 'squad', 'backlog_volume_of_week', 'begin_arrived_week_date','channel']]\n",
    "templ[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now join to daily jobs\n",
    "df = df_daily_jobs.merge(templ, how='left', on=['date', 'squad', 'channel'])\n",
    "df['backlog_volume_daily'] = df['backlog_volume_of_week']*df['wday_percentage']\n",
    "df.tail()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nubank]",
   "language": "python",
   "name": "conda-env-nubank-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "293.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
